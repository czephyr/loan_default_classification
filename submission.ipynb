{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization\n",
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoJrwq1SyNXQ",
        "outputId": "babbf4ce-3e46-4a2d-be62-b4a471bd699f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian-optimization-1.2.0.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-py3-none-any.whl size=11685 sha256=dda9fb2b995fe075dc264bade3593094e0ca9fc9c92ea30d7d0dcb3c60defbfe\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/9b/71/f127d694e02eb40bcf18c7ae9613b88a6be4470f57a8528c5b\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.2.0\n",
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.0.2)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.1.0)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-21.10.1 scikit-optimize-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x5QDyCNs51V"
      },
      "source": [
        "##### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CkBy7u2RsVQT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.api.types import is_string_dtype\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "import warnings\n",
        "\n",
        "from bayes_opt import BayesianOptimization\n",
        "from skopt import BayesSearchCV \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc40PSbHsi59"
      },
      "source": [
        "##### Reading DBs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoM7ZhxSsfnX",
        "outputId": "185eacae-7ecf-42df-8f12-217291b15ee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "dftrain=pd.read_csv('gdrive/My Drive/credimi/train.csv',delimiter=\"|\")\n",
        "test=pd.read_csv('gdrive/My Drive/credimi/test.csv',delimiter=\"|\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XmOPfv0hshLA"
      },
      "outputs": [],
      "source": [
        "test = test.drop([\"sfid\",\"dt_rif\"],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZZ7z-J3RsjkX"
      },
      "outputs": [],
      "source": [
        "target = \"target\"\n",
        "catCols = [x for x in dftrain.columns if (is_string_dtype(dftrain[x]) & (x!=\"dt_rif\") & (x!=\"sfid\")) ]\n",
        "numCols = [x for x in dftrain.columns if (is_numeric_dtype(dftrain[x])) & (x!=\"target\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vNPekUhHslP7"
      },
      "outputs": [],
      "source": [
        "X = dftrain[catCols+numCols]\n",
        "y = dftrain[target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xsw4Ad_Dsm0M"
      },
      "outputs": [],
      "source": [
        "for c in catCols:\n",
        "  test = test.astype({c:\"category\"})\n",
        "for c in catCols:\n",
        "  X = X.astype({c:\"category\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0MvIac-s22C"
      },
      "source": [
        "##### Custom metric definition and printing results utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "URGyBmACyEJl"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "def MoneyScore(y_true,y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return (3*tn - (3*fp) - (50*fn))*1000\n",
        "\n",
        "def credimiScoring(y_true,y_pred):\n",
        "    y_pred = [0  if x<0.2 else 1 for x in y_pred]\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    print(\"True Negative: \"+str(+3*tn*1000))\n",
        "    print(\"False Positive: \"+str(-3*fp*1000))\n",
        "    print(\"False Negative: \"+str(-50*fn*1000))\n",
        "    print(\"Total score: \"+str(MoneyScore(y_true,y_pred)))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plot_confusion_matrix(cm, ['0', '1'],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8Bdu3jlKtFN_"
      },
      "outputs": [],
      "source": [
        "# Could create a mechanism to find the best threshold\n",
        "def credimiMetric(preds: np.ndarray, data: lgb.Dataset, threshold: float=0.2):\n",
        "    label = data.get_label()\n",
        "    pred_label = (preds > threshold).astype(int)\n",
        "    \n",
        "    tn, fp, fn, tp = confusion_matrix(label, pred_label).ravel()\n",
        "\n",
        "    num_zeros = (label == 0).sum()\n",
        "    num_ones = (label == 1).sum()\n",
        "    \n",
        "    worst = (-3*num_zeros) + (-50*num_ones)\n",
        "    best = (3*num_zeros)\n",
        "\n",
        "    score = (((3*tn) - (3*fp) - (50*fn)) - worst) / (best - worst)\n",
        "    \n",
        "    return 'credimi', score, True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUvIM_nStISL"
      },
      "source": [
        "##### Bayesian Hyperparam optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwI7UlehssK4",
        "outputId": "8aa02266-2082-4286-9b82-387cf7f231f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8199  \u001b[0m | \u001b[0m 0.9895  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 20.17   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7931  \u001b[0m | \u001b[0m 0.9964  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 70.77   \u001b[0m | \u001b[0m 12.12   \u001b[0m | \u001b[0m 67.99   \u001b[0m | \u001b[0m 0.258   \u001b[0m |\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8102  \u001b[0m | \u001b[0m 0.8192  \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 56.28   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 54.7    \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 62.09   \u001b[0m | \u001b[0m 0.4252  \u001b[0m |\n",
            "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.827   \u001b[0m | \u001b[95m 0.9281  \u001b[0m | \u001b[95m 0.5869  \u001b[0m | \u001b[95m 0.1144  \u001b[0m | \u001b[95m 87.62   \u001b[0m | \u001b[95m 23.97   \u001b[0m | \u001b[95m 60.78   \u001b[0m | \u001b[95m 32.93   \u001b[0m | \u001b[95m 25.48   \u001b[0m | \u001b[95m 0.8056  \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.811   \u001b[0m | \u001b[0m 0.9946  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 45.14   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8132  \u001b[0m | \u001b[0m 0.8342  \u001b[0m | \u001b[0m 0.8825  \u001b[0m | \u001b[0m 0.3083  \u001b[0m | \u001b[0m 36.27   \u001b[0m | \u001b[0m 24.79   \u001b[0m | \u001b[0m 73.04   \u001b[0m | \u001b[0m 98.73   \u001b[0m | \u001b[0m 24.9    \u001b[0m | \u001b[0m 0.2125  \u001b[0m |\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8217  \u001b[0m | \u001b[0m 0.9291  \u001b[0m | \u001b[0m 0.5085  \u001b[0m | \u001b[0m 0.2182  \u001b[0m | \u001b[0m 41.63   \u001b[0m | \u001b[0m 5.44    \u001b[0m | \u001b[0m 58.69   \u001b[0m | \u001b[0m 52.87   \u001b[0m | \u001b[0m 58.24   \u001b[0m | \u001b[0m 0.3061  \u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8104  \u001b[0m | \u001b[0m 0.978   \u001b[0m | \u001b[0m 0.6757  \u001b[0m | \u001b[0m 0.9534  \u001b[0m | \u001b[0m 47.8    \u001b[0m | \u001b[0m 29.72   \u001b[0m | \u001b[0m 47.15   \u001b[0m | \u001b[0m 72.81   \u001b[0m | \u001b[0m 43.27   \u001b[0m | \u001b[0m 0.917   \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8082  \u001b[0m | \u001b[0m 0.8129  \u001b[0m | \u001b[0m 0.1074  \u001b[0m | \u001b[0m 0.4825  \u001b[0m | \u001b[0m 52.92   \u001b[0m | \u001b[0m 5.791   \u001b[0m | \u001b[0m 74.79   \u001b[0m | \u001b[0m 93.03   \u001b[0m | \u001b[0m 52.8    \u001b[0m | \u001b[0m 0.5479  \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8121  \u001b[0m | \u001b[0m 0.9107  \u001b[0m | \u001b[0m 0.7757  \u001b[0m | \u001b[0m 0.9181  \u001b[0m | \u001b[0m 37.13   \u001b[0m | \u001b[0m 27.67   \u001b[0m | \u001b[0m 61.34   \u001b[0m | \u001b[0m 72.31   \u001b[0m | \u001b[0m 40.61   \u001b[0m | \u001b[0m 0.5334  \u001b[0m |\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.787   \u001b[0m | \u001b[0m 0.8346  \u001b[0m | \u001b[0m 0.411   \u001b[0m | \u001b[0m 0.01544 \u001b[0m | \u001b[0m 31.12   \u001b[0m | \u001b[0m 24.16   \u001b[0m | \u001b[0m 50.98   \u001b[0m | \u001b[0m 67.86   \u001b[0m | \u001b[0m 27.84   \u001b[0m | \u001b[0m 0.8701  \u001b[0m |\n",
            "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8168  \u001b[0m | \u001b[0m 0.8778  \u001b[0m | \u001b[0m 0.1449  \u001b[0m | \u001b[0m 0.4857  \u001b[0m | \u001b[0m 77.35   \u001b[0m | \u001b[0m 17.8    \u001b[0m | \u001b[0m 21.58   \u001b[0m | \u001b[0m 83.91   \u001b[0m | \u001b[0m 32.5    \u001b[0m | \u001b[0m 0.9578  \u001b[0m |\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.809   \u001b[0m | \u001b[0m 0.8058  \u001b[0m | \u001b[0m 0.2753  \u001b[0m | \u001b[0m 0.922   \u001b[0m | \u001b[0m 42.61   \u001b[0m | \u001b[0m 29.98   \u001b[0m | \u001b[0m 53.37   \u001b[0m | \u001b[0m 62.14   \u001b[0m | \u001b[0m 43.34   \u001b[0m | \u001b[0m 0.8881  \u001b[0m |\n",
            "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7967  \u001b[0m | \u001b[0m 0.8226  \u001b[0m | \u001b[0m 0.2348  \u001b[0m | \u001b[0m 0.7086  \u001b[0m | \u001b[0m 85.8    \u001b[0m | \u001b[0m 26.6    \u001b[0m | \u001b[0m 72.68   \u001b[0m | \u001b[0m 16.79   \u001b[0m | \u001b[0m 29.56   \u001b[0m | \u001b[0m 0.4145  \u001b[0m |\n",
            "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.8149  \u001b[0m | \u001b[0m 0.8001  \u001b[0m | \u001b[0m 0.2416  \u001b[0m | \u001b[0m 0.4926  \u001b[0m | \u001b[0m 82.44   \u001b[0m | \u001b[0m 20.04   \u001b[0m | \u001b[0m 47.33   \u001b[0m | \u001b[0m 96.03   \u001b[0m | \u001b[0m 49.52   \u001b[0m | \u001b[0m 0.4531  \u001b[0m |\n",
            "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.804   \u001b[0m | \u001b[0m 0.8692  \u001b[0m | \u001b[0m 0.2294  \u001b[0m | \u001b[0m 0.7799  \u001b[0m | \u001b[0m 38.32   \u001b[0m | \u001b[0m 16.03   \u001b[0m | \u001b[0m 73.95   \u001b[0m | \u001b[0m 43.35   \u001b[0m | \u001b[0m 76.71   \u001b[0m | \u001b[0m 0.5476  \u001b[0m |\n",
            "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.8103  \u001b[0m | \u001b[0m 0.9865  \u001b[0m | \u001b[0m 0.4676  \u001b[0m | \u001b[0m 0.7499  \u001b[0m | \u001b[0m 25.85   \u001b[0m | \u001b[0m 21.15   \u001b[0m | \u001b[0m 49.72   \u001b[0m | \u001b[0m 57.78   \u001b[0m | \u001b[0m 24.3    \u001b[0m | \u001b[0m 0.1039  \u001b[0m |\n",
            "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.8033  \u001b[0m | \u001b[0m 0.9285  \u001b[0m | \u001b[0m 0.1476  \u001b[0m | \u001b[0m 0.7962  \u001b[0m | \u001b[0m 42.86   \u001b[0m | \u001b[0m 28.86   \u001b[0m | \u001b[0m 34.56   \u001b[0m | \u001b[0m 67.26   \u001b[0m | \u001b[0m 40.61   \u001b[0m | \u001b[0m 0.0649  \u001b[0m |\n",
            "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.8085  \u001b[0m | \u001b[0m 0.8427  \u001b[0m | \u001b[0m 0.8454  \u001b[0m | \u001b[0m 0.7872  \u001b[0m | \u001b[0m 65.8    \u001b[0m | \u001b[0m 27.92   \u001b[0m | \u001b[0m 70.87   \u001b[0m | \u001b[0m 87.34   \u001b[0m | \u001b[0m 66.27   \u001b[0m | \u001b[0m 0.7312  \u001b[0m |\n",
            "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.8062  \u001b[0m | \u001b[0m 0.8448  \u001b[0m | \u001b[0m 0.5267  \u001b[0m | \u001b[0m 0.757   \u001b[0m | \u001b[0m 79.36   \u001b[0m | \u001b[0m 17.73   \u001b[0m | \u001b[0m 26.32   \u001b[0m | \u001b[0m 65.58   \u001b[0m | \u001b[0m 28.48   \u001b[0m | \u001b[0m 0.2882  \u001b[0m |\n",
            "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.8246  \u001b[0m | \u001b[0m 0.9387  \u001b[0m | \u001b[0m 0.3624  \u001b[0m | \u001b[0m 0.1433  \u001b[0m | \u001b[0m 35.58   \u001b[0m | \u001b[0m 13.73   \u001b[0m | \u001b[0m 25.14   \u001b[0m | \u001b[0m 47.27   \u001b[0m | \u001b[0m 31.22   \u001b[0m | \u001b[0m 0.4829  \u001b[0m |\n",
            "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.8267  \u001b[0m | \u001b[0m 0.9849  \u001b[0m | \u001b[0m 0.4333  \u001b[0m | \u001b[0m 0.0762  \u001b[0m | \u001b[0m 21.44   \u001b[0m | \u001b[0m 26.08   \u001b[0m | \u001b[0m 75.13   \u001b[0m | \u001b[0m 5.674   \u001b[0m | \u001b[0m 48.97   \u001b[0m | \u001b[0m 0.3406  \u001b[0m |\n",
            "| \u001b[95m 23      \u001b[0m | \u001b[95m 0.8283  \u001b[0m | \u001b[95m 0.8594  \u001b[0m | \u001b[95m 0.8792  \u001b[0m | \u001b[95m 0.2576  \u001b[0m | \u001b[95m 75.94   \u001b[0m | \u001b[95m 15.12   \u001b[0m | \u001b[95m 23.38   \u001b[0m | \u001b[95m 29.57   \u001b[0m | \u001b[95m 53.58   \u001b[0m | \u001b[95m 0.181   \u001b[0m |\n",
            "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.8184  \u001b[0m | \u001b[0m 0.9315  \u001b[0m | \u001b[0m 0.675   \u001b[0m | \u001b[0m 0.08889 \u001b[0m | \u001b[0m 30.04   \u001b[0m | \u001b[0m 11.74   \u001b[0m | \u001b[0m 52.23   \u001b[0m | \u001b[0m 56.76   \u001b[0m | \u001b[0m 24.65   \u001b[0m | \u001b[0m 0.2402  \u001b[0m |\n",
            "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.7353  \u001b[0m | \u001b[0m 0.866   \u001b[0m | \u001b[0m 0.1393  \u001b[0m | \u001b[0m 0.8434  \u001b[0m | \u001b[0m 76.65   \u001b[0m | \u001b[0m 21.02   \u001b[0m | \u001b[0m 61.72   \u001b[0m | \u001b[0m 2.275   \u001b[0m | \u001b[0m 26.75   \u001b[0m | \u001b[0m 0.9233  \u001b[0m |\n",
            "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.8088  \u001b[0m | \u001b[0m 0.8315  \u001b[0m | \u001b[0m 0.3485  \u001b[0m | \u001b[0m 0.8585  \u001b[0m | \u001b[0m 66.41   \u001b[0m | \u001b[0m 22.46   \u001b[0m | \u001b[0m 26.41   \u001b[0m | \u001b[0m 53.61   \u001b[0m | \u001b[0m 70.2    \u001b[0m | \u001b[0m 0.7632  \u001b[0m |\n",
            "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.8078  \u001b[0m | \u001b[0m 0.9495  \u001b[0m | \u001b[0m 0.808   \u001b[0m | \u001b[0m 0.7285  \u001b[0m | \u001b[0m 21.57   \u001b[0m | \u001b[0m 12.55   \u001b[0m | \u001b[0m 74.06   \u001b[0m | \u001b[0m 50.28   \u001b[0m | \u001b[0m 71.19   \u001b[0m | \u001b[0m 0.272   \u001b[0m |\n",
            "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.8068  \u001b[0m | \u001b[0m 0.9239  \u001b[0m | \u001b[0m 0.169   \u001b[0m | \u001b[0m 0.5205  \u001b[0m | \u001b[0m 49.82   \u001b[0m | \u001b[0m 28.38   \u001b[0m | \u001b[0m 44.92   \u001b[0m | \u001b[0m 40.54   \u001b[0m | \u001b[0m 52.8    \u001b[0m | \u001b[0m 0.05939 \u001b[0m |\n",
            "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.8082  \u001b[0m | \u001b[0m 0.9533  \u001b[0m | \u001b[0m 0.5646  \u001b[0m | \u001b[0m 0.9979  \u001b[0m | \u001b[0m 64.45   \u001b[0m | \u001b[0m 21.1    \u001b[0m | \u001b[0m 31.42   \u001b[0m | \u001b[0m 41.85   \u001b[0m | \u001b[0m 75.74   \u001b[0m | \u001b[0m 0.09249 \u001b[0m |\n",
            "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.8224  \u001b[0m | \u001b[0m 0.9065  \u001b[0m | \u001b[0m 0.3196  \u001b[0m | \u001b[0m 0.2825  \u001b[0m | \u001b[0m 24.25   \u001b[0m | \u001b[0m 6.81    \u001b[0m | \u001b[0m 30.15   \u001b[0m | \u001b[0m 44.85   \u001b[0m | \u001b[0m 31.81   \u001b[0m | \u001b[0m 0.5869  \u001b[0m |\n",
            "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.7537  \u001b[0m | \u001b[0m 0.8824  \u001b[0m | \u001b[0m 0.1131  \u001b[0m | \u001b[0m 0.444   \u001b[0m | \u001b[0m 59.7    \u001b[0m | \u001b[0m 21.91   \u001b[0m | \u001b[0m 53.29   \u001b[0m | \u001b[0m 7.976   \u001b[0m | \u001b[0m 58.78   \u001b[0m | \u001b[0m 0.4582  \u001b[0m |\n",
            "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.8182  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.7322  \u001b[0m | \u001b[0m 0.2767  \u001b[0m | \u001b[0m 46.49   \u001b[0m | \u001b[0m 20.43   \u001b[0m | \u001b[0m 66.04   \u001b[0m | \u001b[0m 69.7    \u001b[0m | \u001b[0m 70.94   \u001b[0m | \u001b[0m 0.9967  \u001b[0m |\n",
            "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.8167  \u001b[0m | \u001b[0m 0.8089  \u001b[0m | \u001b[0m 0.8168  \u001b[0m | \u001b[0m 0.2141  \u001b[0m | \u001b[0m 71.57   \u001b[0m | \u001b[0m 18.49   \u001b[0m | \u001b[0m 50.59   \u001b[0m | \u001b[0m 96.26   \u001b[0m | \u001b[0m 65.54   \u001b[0m | \u001b[0m 0.7258  \u001b[0m |\n",
            "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.8122  \u001b[0m | \u001b[0m 0.9268  \u001b[0m | \u001b[0m 0.3273  \u001b[0m | \u001b[0m 0.757   \u001b[0m | \u001b[0m 47.98   \u001b[0m | \u001b[0m 25.1    \u001b[0m | \u001b[0m 31.04   \u001b[0m | \u001b[0m 33.84   \u001b[0m | \u001b[0m 50.52   \u001b[0m | \u001b[0m 0.7226  \u001b[0m |\n",
            "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.8179  \u001b[0m | \u001b[0m 0.8936  \u001b[0m | \u001b[0m 0.5412  \u001b[0m | \u001b[0m 0.4636  \u001b[0m | \u001b[0m 31.9    \u001b[0m | \u001b[0m 15.91   \u001b[0m | \u001b[0m 58.37   \u001b[0m | \u001b[0m 76.11   \u001b[0m | \u001b[0m 30.93   \u001b[0m | \u001b[0m 0.627   \u001b[0m |\n",
            "| \u001b[0m 36      \u001b[0m | \u001b[0m 0.8139  \u001b[0m | \u001b[0m 0.922   \u001b[0m | \u001b[0m 0.6578  \u001b[0m | \u001b[0m 0.5587  \u001b[0m | \u001b[0m 27.84   \u001b[0m | \u001b[0m 26.46   \u001b[0m | \u001b[0m 25.47   \u001b[0m | \u001b[0m 70.72   \u001b[0m | \u001b[0m 77.06   \u001b[0m | \u001b[0m 0.4922  \u001b[0m |\n",
            "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.7932  \u001b[0m | \u001b[0m 0.9397  \u001b[0m | \u001b[0m 0.206   \u001b[0m | \u001b[0m 0.9703  \u001b[0m | \u001b[0m 20.42   \u001b[0m | \u001b[0m 12.29   \u001b[0m | \u001b[0m 31.71   \u001b[0m | \u001b[0m 37.39   \u001b[0m | \u001b[0m 38.95   \u001b[0m | \u001b[0m 0.6832  \u001b[0m |\n",
            "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.8126  \u001b[0m | \u001b[0m 0.919   \u001b[0m | \u001b[0m 0.8574  \u001b[0m | \u001b[0m 0.8455  \u001b[0m | \u001b[0m 46.42   \u001b[0m | \u001b[0m 26.11   \u001b[0m | \u001b[0m 65.21   \u001b[0m | \u001b[0m 76.51   \u001b[0m | \u001b[0m 40.73   \u001b[0m | \u001b[0m 0.9418  \u001b[0m |\n",
            "| \u001b[0m 39      \u001b[0m | \u001b[0m 0.8136  \u001b[0m | \u001b[0m 0.9332  \u001b[0m | \u001b[0m 0.2598  \u001b[0m | \u001b[0m 0.3526  \u001b[0m | \u001b[0m 52.88   \u001b[0m | \u001b[0m 20.64   \u001b[0m | \u001b[0m 47.85   \u001b[0m | \u001b[0m 70.91   \u001b[0m | \u001b[0m 37.67   \u001b[0m | \u001b[0m 0.08266 \u001b[0m |\n",
            "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.7878  \u001b[0m | \u001b[0m 0.9297  \u001b[0m | \u001b[0m 0.2951  \u001b[0m | \u001b[0m 0.7303  \u001b[0m | \u001b[0m 61.57   \u001b[0m | \u001b[0m 12.51   \u001b[0m | \u001b[0m 32.15   \u001b[0m | \u001b[0m 9.283   \u001b[0m | \u001b[0m 44.57   \u001b[0m | \u001b[0m 0.3417  \u001b[0m |\n",
            "| \u001b[0m 41      \u001b[0m | \u001b[0m 0.7861  \u001b[0m | \u001b[0m 0.9297  \u001b[0m | \u001b[0m 0.6878  \u001b[0m | \u001b[0m 0.9653  \u001b[0m | \u001b[0m 34.98   \u001b[0m | \u001b[0m 26.68   \u001b[0m | \u001b[0m 79.48   \u001b[0m | \u001b[0m 1.103   \u001b[0m | \u001b[0m 38.0    \u001b[0m | \u001b[0m 0.5197  \u001b[0m |\n",
            "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.8173  \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.1799  \u001b[0m | \u001b[0m 0.2908  \u001b[0m | \u001b[0m 52.97   \u001b[0m | \u001b[0m 16.27   \u001b[0m | \u001b[0m 70.33   \u001b[0m | \u001b[0m 27.32   \u001b[0m | \u001b[0m 65.54   \u001b[0m | \u001b[0m 0.1512  \u001b[0m |\n",
            "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.8096  \u001b[0m | \u001b[0m 0.9434  \u001b[0m | \u001b[0m 0.4202  \u001b[0m | \u001b[0m 0.5466  \u001b[0m | \u001b[0m 42.08   \u001b[0m | \u001b[0m 17.15   \u001b[0m | \u001b[0m 36.47   \u001b[0m | \u001b[0m 56.95   \u001b[0m | \u001b[0m 42.05   \u001b[0m | \u001b[0m 0.7969  \u001b[0m |\n",
            "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.8157  \u001b[0m | \u001b[0m 0.9257  \u001b[0m | \u001b[0m 0.2328  \u001b[0m | \u001b[0m 0.3798  \u001b[0m | \u001b[0m 34.78   \u001b[0m | \u001b[0m 29.02   \u001b[0m | \u001b[0m 38.2    \u001b[0m | \u001b[0m 84.17   \u001b[0m | \u001b[0m 48.14   \u001b[0m | \u001b[0m 0.9235  \u001b[0m |\n",
            "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.8145  \u001b[0m | \u001b[0m 0.8379  \u001b[0m | \u001b[0m 0.6282  \u001b[0m | \u001b[0m 0.2524  \u001b[0m | \u001b[0m 32.04   \u001b[0m | \u001b[0m 16.13   \u001b[0m | \u001b[0m 78.08   \u001b[0m | \u001b[0m 95.01   \u001b[0m | \u001b[0m 65.85   \u001b[0m | \u001b[0m 0.6043  \u001b[0m |\n",
            "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.7976  \u001b[0m | \u001b[0m 0.8197  \u001b[0m | \u001b[0m 0.1438  \u001b[0m | \u001b[0m 0.9463  \u001b[0m | \u001b[0m 79.8    \u001b[0m | \u001b[0m 22.28   \u001b[0m | \u001b[0m 28.95   \u001b[0m | \u001b[0m 54.8    \u001b[0m | \u001b[0m 62.24   \u001b[0m | \u001b[0m 0.5029  \u001b[0m |\n",
            "| \u001b[0m 47      \u001b[0m | \u001b[0m 0.8022  \u001b[0m | \u001b[0m 0.9313  \u001b[0m | \u001b[0m 0.7119  \u001b[0m | \u001b[0m 0.7804  \u001b[0m | \u001b[0m 61.15   \u001b[0m | \u001b[0m 18.77   \u001b[0m | \u001b[0m 76.94   \u001b[0m | \u001b[0m 24.11   \u001b[0m | \u001b[0m 74.6    \u001b[0m | \u001b[0m 0.9028  \u001b[0m |\n",
            "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.8265  \u001b[0m | \u001b[0m 0.8973  \u001b[0m | \u001b[0m 0.8947  \u001b[0m | \u001b[0m 0.1392  \u001b[0m | \u001b[0m 71.4    \u001b[0m | \u001b[0m 9.994   \u001b[0m | \u001b[0m 75.31   \u001b[0m | \u001b[0m 26.57   \u001b[0m | \u001b[0m 79.18   \u001b[0m | \u001b[0m 0.513   \u001b[0m |\n",
            "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.8223  \u001b[0m | \u001b[0m 0.8569  \u001b[0m | \u001b[0m 0.177   \u001b[0m | \u001b[0m 0.2512  \u001b[0m | \u001b[0m 62.88   \u001b[0m | \u001b[0m 12.98   \u001b[0m | \u001b[0m 55.09   \u001b[0m | \u001b[0m 33.49   \u001b[0m | \u001b[0m 34.9    \u001b[0m | \u001b[0m 0.4618  \u001b[0m |\n",
            "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.8226  \u001b[0m | \u001b[0m 0.8119  \u001b[0m | \u001b[0m 0.8135  \u001b[0m | \u001b[0m 0.261   \u001b[0m | \u001b[0m 68.88   \u001b[0m | \u001b[0m 8.557   \u001b[0m | \u001b[0m 73.87   \u001b[0m | \u001b[0m 26.42   \u001b[0m | \u001b[0m 39.64   \u001b[0m | \u001b[0m 0.5774  \u001b[0m |\n",
            "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.7774  \u001b[0m | \u001b[0m 0.9224  \u001b[0m | \u001b[0m 0.6581  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 77.07   \u001b[0m | \u001b[0m 14.54   \u001b[0m | \u001b[0m 67.04   \u001b[0m | \u001b[0m 43.62   \u001b[0m | \u001b[0m 33.57   \u001b[0m | \u001b[0m 0.5023  \u001b[0m |\n",
            "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.8193  \u001b[0m | \u001b[0m 0.9854  \u001b[0m | \u001b[0m 0.4059  \u001b[0m | \u001b[0m 0.4486  \u001b[0m | \u001b[0m 22.04   \u001b[0m | \u001b[0m 7.478   \u001b[0m | \u001b[0m 33.47   \u001b[0m | \u001b[0m 43.46   \u001b[0m | \u001b[0m 29.88   \u001b[0m | \u001b[0m 0.3598  \u001b[0m |\n",
            "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.8128  \u001b[0m | \u001b[0m 0.8493  \u001b[0m | \u001b[0m 0.2162  \u001b[0m | \u001b[0m 0.5544  \u001b[0m | \u001b[0m 30.84   \u001b[0m | \u001b[0m 10.12   \u001b[0m | \u001b[0m 30.88   \u001b[0m | \u001b[0m 48.27   \u001b[0m | \u001b[0m 27.1    \u001b[0m | \u001b[0m 0.1017  \u001b[0m |\n",
            "| \u001b[0m 54      \u001b[0m | \u001b[0m 0.8081  \u001b[0m | \u001b[0m 0.8212  \u001b[0m | \u001b[0m 0.8772  \u001b[0m | \u001b[0m 0.7176  \u001b[0m | \u001b[0m 79.58   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 27.58   \u001b[0m | \u001b[0m 32.83   \u001b[0m | \u001b[0m 49.81   \u001b[0m | \u001b[0m 0.04258 \u001b[0m |\n",
            "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.8274  \u001b[0m | \u001b[0m 0.8692  \u001b[0m | \u001b[0m 0.4064  \u001b[0m | \u001b[0m 0.3856  \u001b[0m | \u001b[0m 74.78   \u001b[0m | \u001b[0m 13.39   \u001b[0m | \u001b[0m 22.2    \u001b[0m | \u001b[0m 22.05   \u001b[0m | \u001b[0m 50.44   \u001b[0m | \u001b[0m 0.3644  \u001b[0m |\n",
            "| \u001b[0m 56      \u001b[0m | \u001b[0m 0.7798  \u001b[0m | \u001b[0m 0.9049  \u001b[0m | \u001b[0m 0.6279  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 73.25   \u001b[0m | \u001b[0m 17.32   \u001b[0m | \u001b[0m 20.0    \u001b[0m | \u001b[0m 23.99   \u001b[0m | \u001b[0m 58.24   \u001b[0m | \u001b[0m 0.4496  \u001b[0m |\n",
            "| \u001b[0m 57      \u001b[0m | \u001b[0m 0.8175  \u001b[0m | \u001b[0m 0.885   \u001b[0m | \u001b[0m 0.3614  \u001b[0m | \u001b[0m 0.4289  \u001b[0m | \u001b[0m 76.28   \u001b[0m | \u001b[0m 17.89   \u001b[0m | \u001b[0m 23.9    \u001b[0m | \u001b[0m 28.62   \u001b[0m | \u001b[0m 49.43   \u001b[0m | \u001b[0m 0.6806  \u001b[0m |\n",
            "| \u001b[0m 58      \u001b[0m | \u001b[0m 0.8169  \u001b[0m | \u001b[0m 0.8012  \u001b[0m | \u001b[0m 0.1504  \u001b[0m | \u001b[0m 0.2281  \u001b[0m | \u001b[0m 76.44   \u001b[0m | \u001b[0m 10.78   \u001b[0m | \u001b[0m 25.74   \u001b[0m | \u001b[0m 20.05   \u001b[0m | \u001b[0m 50.08   \u001b[0m | \u001b[0m 0.4495  \u001b[0m |\n",
            "| \u001b[0m 59      \u001b[0m | \u001b[0m 0.8187  \u001b[0m | \u001b[0m 0.8893  \u001b[0m | \u001b[0m 0.29    \u001b[0m | \u001b[0m 0.4742  \u001b[0m | \u001b[0m 74.03   \u001b[0m | \u001b[0m 12.24   \u001b[0m | \u001b[0m 25.71   \u001b[0m | \u001b[0m 26.7    \u001b[0m | \u001b[0m 49.26   \u001b[0m | \u001b[0m 0.5047  \u001b[0m |\n",
            "| \u001b[0m 60      \u001b[0m | \u001b[0m 0.801   \u001b[0m | \u001b[0m 0.8957  \u001b[0m | \u001b[0m 0.3407  \u001b[0m | \u001b[0m 0.8647  \u001b[0m | \u001b[0m 74.43   \u001b[0m | \u001b[0m 11.91   \u001b[0m | \u001b[0m 21.25   \u001b[0m | \u001b[0m 32.11   \u001b[0m | \u001b[0m 53.26   \u001b[0m | \u001b[0m 0.1151  \u001b[0m |\n",
            "| \u001b[0m 61      \u001b[0m | \u001b[0m 0.8268  \u001b[0m | \u001b[0m 0.8766  \u001b[0m | \u001b[0m 0.5388  \u001b[0m | \u001b[0m 0.2729  \u001b[0m | \u001b[0m 85.78   \u001b[0m | \u001b[0m 26.18   \u001b[0m | \u001b[0m 57.81   \u001b[0m | \u001b[0m 30.51   \u001b[0m | \u001b[0m 26.35   \u001b[0m | \u001b[0m 0.3527  \u001b[0m |\n",
            "| \u001b[0m 62      \u001b[0m | \u001b[0m 0.8172  \u001b[0m | \u001b[0m 0.8288  \u001b[0m | \u001b[0m 0.1433  \u001b[0m | \u001b[0m 0.1648  \u001b[0m | \u001b[0m 78.31   \u001b[0m | \u001b[0m 14.21   \u001b[0m | \u001b[0m 25.79   \u001b[0m | \u001b[0m 24.07   \u001b[0m | \u001b[0m 50.89   \u001b[0m | \u001b[0m 0.2499  \u001b[0m |\n",
            "| \u001b[0m 63      \u001b[0m | \u001b[0m 0.8022  \u001b[0m | \u001b[0m 0.981   \u001b[0m | \u001b[0m 0.7809  \u001b[0m | \u001b[0m 0.8164  \u001b[0m | \u001b[0m 84.35   \u001b[0m | \u001b[0m 23.83   \u001b[0m | \u001b[0m 61.5    \u001b[0m | \u001b[0m 25.9    \u001b[0m | \u001b[0m 27.53   \u001b[0m | \u001b[0m 0.4155  \u001b[0m |\n",
            "| \u001b[0m 64      \u001b[0m | \u001b[0m 0.8246  \u001b[0m | \u001b[0m 0.915   \u001b[0m | \u001b[0m 0.8657  \u001b[0m | \u001b[0m 0.25    \u001b[0m | \u001b[0m 74.86   \u001b[0m | \u001b[0m 9.659   \u001b[0m | \u001b[0m 78.04   \u001b[0m | \u001b[0m 29.41   \u001b[0m | \u001b[0m 79.91   \u001b[0m | \u001b[0m 0.9732  \u001b[0m |\n",
            "| \u001b[0m 65      \u001b[0m | \u001b[0m 0.7758  \u001b[0m | \u001b[0m 0.8465  \u001b[0m | \u001b[0m 0.3481  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 88.07   \u001b[0m | \u001b[0m 26.86   \u001b[0m | \u001b[0m 56.56   \u001b[0m | \u001b[0m 35.04   \u001b[0m | \u001b[0m 24.83   \u001b[0m | \u001b[0m 0.6383  \u001b[0m |\n",
            "| \u001b[0m 66      \u001b[0m | \u001b[0m 0.8111  \u001b[0m | \u001b[0m 0.8589  \u001b[0m | \u001b[0m 0.5016  \u001b[0m | \u001b[0m 0.5843  \u001b[0m | \u001b[0m 72.23   \u001b[0m | \u001b[0m 12.04   \u001b[0m | \u001b[0m 22.84   \u001b[0m | \u001b[0m 18.54   \u001b[0m | \u001b[0m 53.45   \u001b[0m | \u001b[0m 0.1663  \u001b[0m |\n",
            "| \u001b[0m 67      \u001b[0m | \u001b[0m 0.8101  \u001b[0m | \u001b[0m 0.8591  \u001b[0m | \u001b[0m 0.2148  \u001b[0m | \u001b[0m 0.8227  \u001b[0m | \u001b[0m 35.86   \u001b[0m | \u001b[0m 14.17   \u001b[0m | \u001b[0m 26.74   \u001b[0m | \u001b[0m 46.92   \u001b[0m | \u001b[0m 33.86   \u001b[0m | \u001b[0m 0.5314  \u001b[0m |\n",
            "| \u001b[95m 68      \u001b[0m | \u001b[95m 0.8305  \u001b[0m | \u001b[95m 0.8774  \u001b[0m | \u001b[95m 0.7611  \u001b[0m | \u001b[95m 0.1143  \u001b[0m | \u001b[95m 22.29   \u001b[0m | \u001b[95m 24.18   \u001b[0m | \u001b[95m 73.08   \u001b[0m | \u001b[95m 5.446   \u001b[0m | \u001b[95m 47.13   \u001b[0m | \u001b[95m 0.09516 \u001b[0m |\n",
            "| \u001b[0m 69      \u001b[0m | \u001b[0m 0.8218  \u001b[0m | \u001b[0m 0.9622  \u001b[0m | \u001b[0m 0.4031  \u001b[0m | \u001b[0m 0.4295  \u001b[0m | \u001b[0m 72.03   \u001b[0m | \u001b[0m 13.6    \u001b[0m | \u001b[0m 21.91   \u001b[0m | \u001b[0m 27.12   \u001b[0m | \u001b[0m 50.77   \u001b[0m | \u001b[0m 0.9627  \u001b[0m |\n",
            "| \u001b[0m 70      \u001b[0m | \u001b[0m 0.8182  \u001b[0m | \u001b[0m 0.8903  \u001b[0m | \u001b[0m 0.7117  \u001b[0m | \u001b[0m 0.3481  \u001b[0m | \u001b[0m 22.53   \u001b[0m | \u001b[0m 24.47   \u001b[0m | \u001b[0m 71.88   \u001b[0m | \u001b[0m 6.53    \u001b[0m | \u001b[0m 50.36   \u001b[0m | \u001b[0m 0.0709  \u001b[0m |\n",
            "| \u001b[0m 71      \u001b[0m | \u001b[0m 0.802   \u001b[0m | \u001b[0m 0.9188  \u001b[0m | \u001b[0m 0.1664  \u001b[0m | \u001b[0m 0.8941  \u001b[0m | \u001b[0m 78.68   \u001b[0m | \u001b[0m 13.51   \u001b[0m | \u001b[0m 23.4    \u001b[0m | \u001b[0m 27.61   \u001b[0m | \u001b[0m 53.45   \u001b[0m | \u001b[0m 0.2712  \u001b[0m |\n",
            "| \u001b[0m 72      \u001b[0m | \u001b[0m 0.7752  \u001b[0m | \u001b[0m 0.8119  \u001b[0m | \u001b[0m 0.252   \u001b[0m | \u001b[0m 0.9782  \u001b[0m | \u001b[0m 21.38   \u001b[0m | \u001b[0m 23.86   \u001b[0m | \u001b[0m 71.73   \u001b[0m | \u001b[0m 8.913   \u001b[0m | \u001b[0m 45.65   \u001b[0m | \u001b[0m 0.4066  \u001b[0m |\n",
            "| \u001b[0m 73      \u001b[0m | \u001b[0m 0.8082  \u001b[0m | \u001b[0m 0.9017  \u001b[0m | \u001b[0m 0.668   \u001b[0m | \u001b[0m 0.8075  \u001b[0m | \u001b[0m 77.71   \u001b[0m | \u001b[0m 19.63   \u001b[0m | \u001b[0m 24.24   \u001b[0m | \u001b[0m 30.48   \u001b[0m | \u001b[0m 51.6    \u001b[0m | \u001b[0m 0.6519  \u001b[0m |\n",
            "| \u001b[0m 74      \u001b[0m | \u001b[0m 0.8157  \u001b[0m | \u001b[0m 0.8651  \u001b[0m | \u001b[0m 0.8172  \u001b[0m | \u001b[0m 0.4696  \u001b[0m | \u001b[0m 69.0    \u001b[0m | \u001b[0m 8.334   \u001b[0m | \u001b[0m 73.37   \u001b[0m | \u001b[0m 22.24   \u001b[0m | \u001b[0m 42.36   \u001b[0m | \u001b[0m 0.2811  \u001b[0m |\n",
            "| \u001b[0m 75      \u001b[0m | \u001b[0m 0.8044  \u001b[0m | \u001b[0m 0.9932  \u001b[0m | \u001b[0m 0.6889  \u001b[0m | \u001b[0m 0.8671  \u001b[0m | \u001b[0m 73.7    \u001b[0m | \u001b[0m 13.86   \u001b[0m | \u001b[0m 22.96   \u001b[0m | \u001b[0m 25.53   \u001b[0m | \u001b[0m 50.21   \u001b[0m | \u001b[0m 0.5016  \u001b[0m |\n",
            "| \u001b[0m 76      \u001b[0m | \u001b[0m 0.7789  \u001b[0m | \u001b[0m 0.9324  \u001b[0m | \u001b[0m 0.4089  \u001b[0m | \u001b[0m 0.9028  \u001b[0m | \u001b[0m 24.49   \u001b[0m | \u001b[0m 24.33   \u001b[0m | \u001b[0m 75.92   \u001b[0m | \u001b[0m 4.262   \u001b[0m | \u001b[0m 48.15   \u001b[0m | \u001b[0m 0.2642  \u001b[0m |\n",
            "| \u001b[0m 77      \u001b[0m | \u001b[0m 0.8274  \u001b[0m | \u001b[0m 0.8799  \u001b[0m | \u001b[0m 0.3081  \u001b[0m | \u001b[0m 0.2283  \u001b[0m | \u001b[0m 49.99   \u001b[0m | \u001b[0m 7.967   \u001b[0m | \u001b[0m 23.79   \u001b[0m | \u001b[0m 39.6    \u001b[0m | \u001b[0m 26.51   \u001b[0m | \u001b[0m 0.3114  \u001b[0m |\n",
            "| \u001b[0m 78      \u001b[0m | \u001b[0m 0.8176  \u001b[0m | \u001b[0m 0.89    \u001b[0m | \u001b[0m 0.384   \u001b[0m | \u001b[0m 0.2804  \u001b[0m | \u001b[0m 36.28   \u001b[0m | \u001b[0m 16.87   \u001b[0m | \u001b[0m 47.5    \u001b[0m | \u001b[0m 57.74   \u001b[0m | \u001b[0m 48.99   \u001b[0m | \u001b[0m 0.04842 \u001b[0m |\n",
            "| \u001b[0m 79      \u001b[0m | \u001b[0m 0.8287  \u001b[0m | \u001b[0m 0.9658  \u001b[0m | \u001b[0m 0.6663  \u001b[0m | \u001b[0m 0.1328  \u001b[0m | \u001b[0m 53.99   \u001b[0m | \u001b[0m 21.93   \u001b[0m | \u001b[0m 65.97   \u001b[0m | \u001b[0m 6.297   \u001b[0m | \u001b[0m 44.1    \u001b[0m | \u001b[0m 0.4984  \u001b[0m |\n",
            "| \u001b[0m 80      \u001b[0m | \u001b[0m 0.8139  \u001b[0m | \u001b[0m 0.8722  \u001b[0m | \u001b[0m 0.5826  \u001b[0m | \u001b[0m 0.571   \u001b[0m | \u001b[0m 72.93   \u001b[0m | \u001b[0m 22.97   \u001b[0m | \u001b[0m 44.77   \u001b[0m | \u001b[0m 63.66   \u001b[0m | \u001b[0m 42.1    \u001b[0m | \u001b[0m 0.1064  \u001b[0m |\n",
            "| \u001b[0m 81      \u001b[0m | \u001b[0m 0.8185  \u001b[0m | \u001b[0m 0.9874  \u001b[0m | \u001b[0m 0.5329  \u001b[0m | \u001b[0m 0.479   \u001b[0m | \u001b[0m 70.51   \u001b[0m | \u001b[0m 18.4    \u001b[0m | \u001b[0m 72.82   \u001b[0m | \u001b[0m 83.67   \u001b[0m | \u001b[0m 56.62   \u001b[0m | \u001b[0m 0.3989  \u001b[0m |\n",
            "| \u001b[0m 82      \u001b[0m | \u001b[0m 0.8272  \u001b[0m | \u001b[0m 0.8575  \u001b[0m | \u001b[0m 0.3582  \u001b[0m | \u001b[0m 0.1277  \u001b[0m | \u001b[0m 67.74   \u001b[0m | \u001b[0m 25.33   \u001b[0m | \u001b[0m 59.94   \u001b[0m | \u001b[0m 12.94   \u001b[0m | \u001b[0m 78.52   \u001b[0m | \u001b[0m 0.3001  \u001b[0m |\n",
            "| \u001b[0m 83      \u001b[0m | \u001b[0m 0.8193  \u001b[0m | \u001b[0m 0.8364  \u001b[0m | \u001b[0m 0.8332  \u001b[0m | \u001b[0m 0.3462  \u001b[0m | \u001b[0m 78.09   \u001b[0m | \u001b[0m 11.45   \u001b[0m | \u001b[0m 59.65   \u001b[0m | \u001b[0m 39.06   \u001b[0m | \u001b[0m 53.28   \u001b[0m | \u001b[0m 0.151   \u001b[0m |\n",
            "| \u001b[0m 84      \u001b[0m | \u001b[0m 0.8162  \u001b[0m | \u001b[0m 0.8158  \u001b[0m | \u001b[0m 0.6129  \u001b[0m | \u001b[0m 0.8195  \u001b[0m | \u001b[0m 44.37   \u001b[0m | \u001b[0m 12.0    \u001b[0m | \u001b[0m 38.56   \u001b[0m | \u001b[0m 84.89   \u001b[0m | \u001b[0m 71.53   \u001b[0m | \u001b[0m 0.02971 \u001b[0m |\n",
            "| \u001b[0m 85      \u001b[0m | \u001b[0m 0.8219  \u001b[0m | \u001b[0m 0.8871  \u001b[0m | \u001b[0m 0.8467  \u001b[0m | \u001b[0m 0.2267  \u001b[0m | \u001b[0m 36.91   \u001b[0m | \u001b[0m 7.142   \u001b[0m | \u001b[0m 58.63   \u001b[0m | \u001b[0m 12.58   \u001b[0m | \u001b[0m 49.53   \u001b[0m | \u001b[0m 0.02921 \u001b[0m |\n",
            "| \u001b[0m 86      \u001b[0m | \u001b[0m 0.8147  \u001b[0m | \u001b[0m 0.8289  \u001b[0m | \u001b[0m 0.1362  \u001b[0m | \u001b[0m 0.4469  \u001b[0m | \u001b[0m 70.06   \u001b[0m | \u001b[0m 15.94   \u001b[0m | \u001b[0m 77.91   \u001b[0m | \u001b[0m 90.88   \u001b[0m | \u001b[0m 38.97   \u001b[0m | \u001b[0m 0.9746  \u001b[0m |\n",
            "| \u001b[0m 87      \u001b[0m | \u001b[0m 0.8059  \u001b[0m | \u001b[0m 0.9011  \u001b[0m | \u001b[0m 0.7901  \u001b[0m | \u001b[0m 0.7355  \u001b[0m | \u001b[0m 56.86   \u001b[0m | \u001b[0m 6.76    \u001b[0m | \u001b[0m 33.47   \u001b[0m | \u001b[0m 56.07   \u001b[0m | \u001b[0m 55.53   \u001b[0m | \u001b[0m 0.1941  \u001b[0m |\n",
            "| \u001b[0m 88      \u001b[0m | \u001b[0m 0.8109  \u001b[0m | \u001b[0m 0.9611  \u001b[0m | \u001b[0m 0.4661  \u001b[0m | \u001b[0m 0.6322  \u001b[0m | \u001b[0m 65.19   \u001b[0m | \u001b[0m 19.08   \u001b[0m | \u001b[0m 36.25   \u001b[0m | \u001b[0m 36.03   \u001b[0m | \u001b[0m 69.49   \u001b[0m | \u001b[0m 0.6569  \u001b[0m |\n",
            "| \u001b[0m 89      \u001b[0m | \u001b[0m 0.8208  \u001b[0m | \u001b[0m 0.9743  \u001b[0m | \u001b[0m 0.7061  \u001b[0m | \u001b[0m 0.1128  \u001b[0m | \u001b[0m 20.13   \u001b[0m | \u001b[0m 23.85   \u001b[0m | \u001b[0m 76.5    \u001b[0m | \u001b[0m 51.68   \u001b[0m | \u001b[0m 55.44   \u001b[0m | \u001b[0m 0.0369  \u001b[0m |\n",
            "| \u001b[0m 90      \u001b[0m | \u001b[0m 0.8134  \u001b[0m | \u001b[0m 0.9269  \u001b[0m | \u001b[0m 0.2445  \u001b[0m | \u001b[0m 0.6213  \u001b[0m | \u001b[0m 27.88   \u001b[0m | \u001b[0m 22.99   \u001b[0m | \u001b[0m 52.73   \u001b[0m | \u001b[0m 64.2    \u001b[0m | \u001b[0m 79.54   \u001b[0m | \u001b[0m 0.05837 \u001b[0m |\n",
            "| \u001b[0m 91      \u001b[0m | \u001b[0m 0.8151  \u001b[0m | \u001b[0m 0.9663  \u001b[0m | \u001b[0m 0.6311  \u001b[0m | \u001b[0m 0.7082  \u001b[0m | \u001b[0m 31.3    \u001b[0m | \u001b[0m 25.42   \u001b[0m | \u001b[0m 76.59   \u001b[0m | \u001b[0m 53.18   \u001b[0m | \u001b[0m 48.67   \u001b[0m | \u001b[0m 0.9309  \u001b[0m |\n",
            "| \u001b[0m 92      \u001b[0m | \u001b[0m 0.8104  \u001b[0m | \u001b[0m 0.9629  \u001b[0m | \u001b[0m 0.393   \u001b[0m | \u001b[0m 0.6778  \u001b[0m | \u001b[0m 86.8    \u001b[0m | \u001b[0m 15.33   \u001b[0m | \u001b[0m 20.62   \u001b[0m | \u001b[0m 95.84   \u001b[0m | \u001b[0m 68.68   \u001b[0m | \u001b[0m 0.03825 \u001b[0m |\n",
            "| \u001b[0m 93      \u001b[0m | \u001b[0m 0.8107  \u001b[0m | \u001b[0m 0.9632  \u001b[0m | \u001b[0m 0.565   \u001b[0m | \u001b[0m 0.04179 \u001b[0m | \u001b[0m 39.37   \u001b[0m | \u001b[0m 29.16   \u001b[0m | \u001b[0m 78.3    \u001b[0m | \u001b[0m 46.89   \u001b[0m | \u001b[0m 39.93   \u001b[0m | \u001b[0m 0.9232  \u001b[0m |\n",
            "| \u001b[0m 94      \u001b[0m | \u001b[0m 0.8191  \u001b[0m | \u001b[0m 0.8108  \u001b[0m | \u001b[0m 0.882   \u001b[0m | \u001b[0m 0.2521  \u001b[0m | \u001b[0m 41.95   \u001b[0m | \u001b[0m 29.38   \u001b[0m | \u001b[0m 71.09   \u001b[0m | \u001b[0m 76.44   \u001b[0m | \u001b[0m 47.22   \u001b[0m | \u001b[0m 0.8764  \u001b[0m |\n",
            "| \u001b[0m 95      \u001b[0m | \u001b[0m 0.8154  \u001b[0m | \u001b[0m 0.9238  \u001b[0m | \u001b[0m 0.5891  \u001b[0m | \u001b[0m 0.3304  \u001b[0m | \u001b[0m 51.34   \u001b[0m | \u001b[0m 5.908   \u001b[0m | \u001b[0m 26.15   \u001b[0m | \u001b[0m 82.59   \u001b[0m | \u001b[0m 68.54   \u001b[0m | \u001b[0m 0.3541  \u001b[0m |\n",
            "| \u001b[0m 96      \u001b[0m | \u001b[0m 0.8143  \u001b[0m | \u001b[0m 0.8736  \u001b[0m | \u001b[0m 0.1209  \u001b[0m | \u001b[0m 0.2849  \u001b[0m | \u001b[0m 62.0    \u001b[0m | \u001b[0m 20.59   \u001b[0m | \u001b[0m 67.46   \u001b[0m | \u001b[0m 45.59   \u001b[0m | \u001b[0m 73.13   \u001b[0m | \u001b[0m 0.9326  \u001b[0m |\n",
            "| \u001b[0m 97      \u001b[0m | \u001b[0m 0.8135  \u001b[0m | \u001b[0m 0.8151  \u001b[0m | \u001b[0m 0.4383  \u001b[0m | \u001b[0m 0.5509  \u001b[0m | \u001b[0m 48.9    \u001b[0m | \u001b[0m 16.01   \u001b[0m | \u001b[0m 34.74   \u001b[0m | \u001b[0m 22.15   \u001b[0m | \u001b[0m 43.22   \u001b[0m | \u001b[0m 0.6135  \u001b[0m |\n",
            "| \u001b[0m 98      \u001b[0m | \u001b[0m 0.8282  \u001b[0m | \u001b[0m 0.8467  \u001b[0m | \u001b[0m 0.666   \u001b[0m | \u001b[0m 0.1965  \u001b[0m | \u001b[0m 36.64   \u001b[0m | \u001b[0m 13.8    \u001b[0m | \u001b[0m 31.74   \u001b[0m | \u001b[0m 32.13   \u001b[0m | \u001b[0m 69.39   \u001b[0m | \u001b[0m 0.6059  \u001b[0m |\n",
            "| \u001b[0m 99      \u001b[0m | \u001b[0m 0.8146  \u001b[0m | \u001b[0m 0.8112  \u001b[0m | \u001b[0m 0.7006  \u001b[0m | \u001b[0m 0.3515  \u001b[0m | \u001b[0m 28.03   \u001b[0m | \u001b[0m 21.42   \u001b[0m | \u001b[0m 22.53   \u001b[0m | \u001b[0m 93.43   \u001b[0m | \u001b[0m 42.05   \u001b[0m | \u001b[0m 0.3599  \u001b[0m |\n",
            "| \u001b[0m 100     \u001b[0m | \u001b[0m 0.8098  \u001b[0m | \u001b[0m 0.9674  \u001b[0m | \u001b[0m 0.1429  \u001b[0m | \u001b[0m 0.613   \u001b[0m | \u001b[0m 76.69   \u001b[0m | \u001b[0m 15.12   \u001b[0m | \u001b[0m 56.57   \u001b[0m | \u001b[0m 77.89   \u001b[0m | \u001b[0m 37.27   \u001b[0m | \u001b[0m 0.06455 \u001b[0m |\n",
            "=====================================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Source https://www.kaggle.com/code/somang1418/tuning-hyperparameters-under-10-minutes-lgbm/notebook\n",
        "\n",
        "def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=6,n_estimators=10000, output_process=False):\n",
        "  train_data = lgb.Dataset(data=X, label=y, free_raw_data=False,categorical_feature=catCols,)\n",
        "\n",
        "  def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n",
        "    params = {'application':'binary', 'metric':'auc',\"is_unbalanced\":True}\n",
        "    params['learning_rate'] = max(min(learning_rate, 1), 0)\n",
        "    params[\"num_leaves\"] = int(round(num_leaves))\n",
        "    params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
        "    params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
        "    params['max_depth'] = int(round(max_depth))\n",
        "    params['max_bin'] = int(round(max_depth))\n",
        "    params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
        "    params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
        "    params['subsample'] = max(min(subsample, 1), 0)\n",
        "        \n",
        "    cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=None,feval=credimiMetric)\n",
        "    return max(cv_result['credimi-mean'])\n",
        "     \n",
        "  lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0),\n",
        "                                          'num_leaves': (24, 80),\n",
        "                                          'feature_fraction': (0.1, 0.9),\n",
        "                                          'bagging_fraction': (0.8, 1),\n",
        "                                          'max_depth': (5, 30),\n",
        "                                          'max_bin':(20,90),\n",
        "                                          'min_data_in_leaf': (20, 80),\n",
        "                                          'min_sum_hessian_in_leaf':(0,100),\n",
        "                                          'subsample': (0.01, 1.0)}, random_state=200)\n",
        "\n",
        "    \n",
        "  lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
        "  \n",
        "  model_auc=[]\n",
        "  for model in range(len( lgbBO.res)):\n",
        "      model_auc.append(lgbBO.res[model]['target'])\n",
        "  \n",
        "  # return best parameters\n",
        "  return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    # init_round=50, opt_round=50 during actual testing \n",
        "    opt_params = bayes_parameter_opt_lgb(X, y, init_round=50, opt_round=50, n_folds=3, random_seed=6,n_estimators=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OweXO_o7sxBT",
        "outputId": "72fe23e9-5815-4c47-a2ea-6199bb4bba4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bagging_fraction': 0.8773888983810538,\n",
              " 'boost_from_average': False,\n",
              " 'feature_fraction': 0.7610913709177939,\n",
              " 'is_unbalance': True,\n",
              " 'learning_rate': 0.11431478403733293,\n",
              " 'max_bin': 22,\n",
              " 'max_depth': 24,\n",
              " 'metric': None,\n",
              " 'min_data_in_leaf': 73,\n",
              " 'min_sum_hessian_in_leaf': 5.445503374547966,\n",
              " 'num_leaves': 47,\n",
              " 'objective': 'binary',\n",
              " 'subsample': 0.09516083782499006}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
        "opt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\n",
        "opt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\n",
        "opt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\n",
        "opt_params[1]['objective']='binary'\n",
        "opt_params[1]['metric']='auc'\n",
        "opt_params[1]['is_unbalance']=True\n",
        "opt_params[1]['boost_from_average']=False\n",
        "opt_params[1][\"metric\"] = None\n",
        "found_opt_params=opt_params[1]\n",
        "found_opt_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py6rWdNdtQmd"
      },
      "source": [
        "##### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qOaTKyv1e45E",
        "outputId": "750a5e9e-7f01-4a63-ba4d-c22099c0623f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0\n",
            "Training until validation scores don't improve for 250 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[39]\ttraining's binary_logloss: 0.192561\ttraining's credimi: 0.899032\tvalid_1's binary_logloss: 0.25151\tvalid_1's credimi: 0.852703\n",
            "Fold 1\n",
            "Training until validation scores don't improve for 250 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[38]\ttraining's binary_logloss: 0.190199\ttraining's credimi: 0.901369\tvalid_1's binary_logloss: 0.296538\tvalid_1's credimi: 0.829289\n",
            "Fold 2\n",
            "Training until validation scores don't improve for 250 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[61]\ttraining's binary_logloss: 0.122837\ttraining's credimi: 0.938895\tvalid_1's binary_logloss: 0.254617\tvalid_1's credimi: 0.827799\n",
            "Fold 3\n",
            "Training until validation scores don't improve for 250 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[147]\ttraining's binary_logloss: 0.0318787\ttraining's credimi: 0.991079\tvalid_1's binary_logloss: 0.186177\tvalid_1's credimi: 0.894636\n",
            "Fold 4\n",
            "Training until validation scores don't improve for 250 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[75]\ttraining's binary_logloss: 0.0963474\ttraining's credimi: 0.951711\tvalid_1's binary_logloss: 0.221461\tvalid_1's credimi: 0.881226\n",
            "Fold 5\n",
            "Training until validation scores don't improve for 250 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[61]\ttraining's binary_logloss: 0.122121\ttraining's credimi: 0.937479\tvalid_1's binary_logloss: 0.266943\tvalid_1's credimi: 0.861218\n",
            "Fold 6\n",
            "Training until validation scores don't improve for 250 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[134]\ttraining's binary_logloss: 0.0377547\ttraining's credimi: 0.987815\tvalid_1's binary_logloss: 0.210403\tvalid_1's credimi: 0.883475\n",
            "Fold 7\n",
            "Training until validation scores don't improve for 250 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's binary_logloss: 0.160798\ttraining's credimi: 0.91669\tvalid_1's binary_logloss: 0.250058\tvalid_1's credimi: 0.850212\n",
            "Fold 8\n",
            "Training until validation scores don't improve for 250 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[102]\ttraining's binary_logloss: 0.0615661\ttraining's credimi: 0.973434\tvalid_1's binary_logloss: 0.207575\tvalid_1's credimi: 0.889831\n",
            "Fold 9\n",
            "Training until validation scores don't improve for 250 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[94]\ttraining's binary_logloss: 0.0698052\ttraining's credimi: 0.969255\tvalid_1's binary_logloss: 0.216707\tvalid_1's credimi: 0.887924\n",
            "True Negative: 15654000\n",
            "False Positive: -3564000\n",
            "False Negative: -5500000\n",
            "Total score: 6590000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.81      0.89      6406\n",
            "         1.0       0.46      0.90      0.61      1114\n",
            "\n",
            "    accuracy                           0.83      7520\n",
            "   macro avg       0.72      0.86      0.75      7520\n",
            "weighted avg       0.90      0.83      0.85      7520\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "# Source https://www.kaggle.com/code/somang1418/tuning-hyperparameters-under-10-minutes-lgbm/notebook\n",
        "\n",
        "target=y\n",
        "features= catCols+numCols\n",
        "\n",
        "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=31416)\n",
        "oof = np.zeros(len(X))\n",
        "predictions = np.zeros(len(test))\n",
        "feature_importance_df = pd.DataFrame()\n",
        "with warnings.catch_warnings():\n",
        "  warnings.simplefilter(\"ignore\")\n",
        "  for fold_, (trn_idx, val_idx) in enumerate(folds.split(X.values, target.values)):\n",
        "    print(\"Fold {}\".format(fold_))\n",
        "    trn_data = lgb.Dataset(X.iloc[trn_idx][features], label=target.iloc[trn_idx])\n",
        "    val_data = lgb.Dataset(X.iloc[val_idx][features], label=target.iloc[val_idx])\n",
        "\n",
        "    num_round = 15000\n",
        "    clf = lgb.train(found_opt_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 250,feval=credimiMetric)\n",
        "    oof[val_idx] = clf.predict(X.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
        "    \n",
        "    fold_importance_df = pd.DataFrame()\n",
        "    fold_importance_df[\"Feature\"] = features\n",
        "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
        "    fold_importance_df[\"fold\"] = fold_ + 1\n",
        "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
        "    \n",
        "    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
        "\n",
        "  print(credimiScoring(target, oof))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RevJ4vTTtZf-"
      },
      "source": [
        "##### Feature importance graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqsWGGb0tciC"
      },
      "outputs": [],
      "source": [
        "# Source https://www.kaggle.com/code/somang1418/tuning-hyperparameters-under-10-minutes-lgbm/notebook\n",
        "\n",
        "cols = (feature_importance_df[[\"Feature\", \"importance\"]].groupby(\"Feature\").mean().sort_values(by=\"importance\", ascending=False)[:20].index)\n",
        "\n",
        "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
        "\n",
        "plt.figure(figsize=(20,28))\n",
        "sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
        "plt.title('Features importance (averaged/folds)')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx9B1qCbtdS9"
      },
      "source": [
        "##### Creating Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF_13Mcotpbk"
      },
      "outputs": [],
      "source": [
        "testId=pd.read_csv('gdrive/My Drive/credimi/test.csv',delimiter=\"|\")\n",
        "sol = pd.concat([testId[\"sfid\"],pd.DataFrame([0  if x<0.05 else 1 for x in predictions] )],axis=1)\n",
        "sol = sol.set_index(\"sfid\")\n",
        "sol.columns=[\"label\"]\n",
        "sol.to_csv(\"final.csv\")\n",
        "\n",
        "with open('final.csv', 'r+') as f:\n",
        "  content = f.read()\n",
        "  f.seek(0)\n",
        "  f.truncate()\n",
        "  f.write(content.replace(',', '|'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Submission Funicello.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}